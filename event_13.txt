2017-06-09 14:04:52.117647
one vs all ; one vs one@@@I am not 100% sure what do those two terms mean@@@One-vs-all means that for each class you train a binary classifier that classifies that class vs. all others. Like cat vs. not cat, dog vs. not dog, etc. At prediction time you get a score from each of those classifiers and pick the highest score. One vs one means you learn a classifier between every pair of classes. This is more work because you need to train $$\mathcal{O}(k^2)$$ binary classifiers if you have $$k$$ classes.@@@2017-06-08T23:51:47Z@@@hw3 student
A3 Q4.1 Question about gradient of logistic regression@@@In order to find the min loss, I guess I need a funObj in my logLinearClassifier to use findMin. Since we have multi-class now the loss function I got is : $$\displaystyle f(w)=\sum_{i=1}^N \sum_{c=1}^C log(1+exp(-y_{ic}W_c^TX_i))$$ But I have trouble finding the gradient of this function, can someone give me some hints?Thanks!!@@@Never mind, I shouldn't calculate the loss function in the way I posted above. Each class should be treated independently.@@@2017-06-09T07:41:56Z@@@hw3 student
Why L2 does not do feature selection@@@Hi, I wonder why L2 does not do feature selection. Isn't having more non-zero term in w would also increases the regularization term and hence the model would be penalized? Thanks!@@@The penalization is bearable as long as w_i is small. More importantly, the minimum doesn't occur at w_i = 0. The L2 regularization, even for super large values of lambda, would only pull you asymptotically close to 0 but never quite reach it. If w_i isn't 0, then it remains selected.@@@2017-06-09T03:40:02Z@@@other student
Incorrect Assignment Deadline for Assignment 2?@@@I noticed in the grade report for assignment 2 that the due that was "2017-02-05 23:59" instead of"2017-05-30 23:59" and got the "Late: Assignment not counted for credit message". My assignment was submitted 6 mins before the correct deadline (30th May 11:59pm). Can someone take a look into that please? (I'm assuming that the grades are calculated automatically from github grade reports). Thank you.@@@This is a mistake. I believe they will fix it soon.@@@2017-06-09T16:00:47Z@@@hw2 other student
Markdown Matrices@@@Tryting to make matrices in my .md file. Anyone have a way to make nice looking matrices. Is there an html tag?@@@You can write math equations in .md the same way as you do in latex. The markdown file linked below contains code that results in nice looking matrices: md_test.md Once you download it, run in terminal the following command to compile the file into pdf: `pandocmd_test.md -o md_test.pdf` You should get a pdf that looks like this:@@@2017-06-10T01:43:55Z@@@other student
Different answers for 3.3@@@My partner and I both implemented 3.3 and we got different answers, we checked the implementation and it was identical.It was all the same up until the last selected feature. What could the reason be? Is it because I'm on mac and my partner is on windows?@@@@@@2017-06-09T20:05:55Z@@@hw3 student
Wikipedia Cross-Validation/Monte-Carlo Cross-Validation@@@https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation This seems to differ from the slides which says k-fold randomizes splits each run. Whereas wikipedia says they are fixed and just a different one is used for validation each time?@@@They are fixed. But you can/should do a randomization step to begin with. Which is the same as picking the folds randomly. But once the $$k$$ folds are picked then you keep them througought the $$k$$ training/prediction phases.@@@2017-06-09T02:53:11Z@@@hw3 student
Q4.4 Code@@@Hello, My code seems complete to me, butI cant figure out where could I possibly have the issue, (I opened agithub issue for instructors) but meanwhile, did anyone ever get results like this? Exception: User and numerical derivatives differ: [ 0. 0. 0. 0. 0.] <-- theirs [ 78. -32.  10. -142.  86.] <---- mine If yes,where did you realize the issue was and how did u go about solving it?? Any tips or help would be greatly appreciated, thanks so much <3 p.s. The way I calculate the gradient, isI iterate through the rows and then columns of the matrix that stores output of the gradient, and then i calculate the derived formula for each one.@@@Think theres something weird going on with your function, since that's what they use to estimate your gradient. Not sure what.@@@2017-06-10T03:42:00Z@@@hw3 student
A3 3.2@@@Question 3.2 says that the findMinL1 function implements the non-differential part of the objective function. Does it implement this for both f and g?@@@Yes.@@@2017-06-09T21:07:10Z@@@hw3 student
A3 Q4.4@@@AfterI have derived the gradient of the loss function I am confused how I should minimize it to find the best W? We used findMin when w was a vector, but now W has more than one column, which is not acceptable by findMin. And I think we can't minimize with respect to each column of W separately, because that's the point to consider W as a whole.@@@Just reshape your W into a column vector w, worked for me.@@@2017-06-09T03:13:56Z@@@hw3 student
A3 Q3.3@@@I'm having difficulty figuring out how to compute the score once fitting selected_new. I know we have to compute the score using the function given in the pdf; however, I am having trouble figuring how to do so after fitting the data. The minimize function returns an array and a float (i.e. [0.01.. , 0.02...] and 340.12....) My intuition, after looking at findMin.py, is that the float that is returned is the value from our summation over our features (f) and the array is our w. Since we're using L0, would that mean we use np.count_nonzero(w), multiply that count by lambda and then add to f to receive our score? And if so, our best feature is that w array and our min loss is score? Or am I completely off?@@@Sounds about right. the findMin function returns the left-hand term of the loss, and you manually add in the L0 regularization term by counting the number of nonzero entries in $$w$$ (which should also just be the length of selected_new ).@@@2017-06-09T01:00:42Z@@@hw3 student
Q2.2 - Singular Matrix Error@@@Hello all, For my Q2.2, I have randomly shuffled my data and am trying to apply the LeastSquaresRBF model to it. However, a Singular matrix error appears. I've checked the dimensions of my arrays and they seem to line up. Did anyone have the same problem as me for this question? Thanks in advance for any help.@@@Probably duplicated a column somehow? Just guessing...@@@2017-06-10T04:28:56Z@@@hw3 student
Q4.3 Massive Struggles with Taking the Derivative@@@Is it simply just a long process or am I missing some insight or trick? EDIT: To clarify I meant the "algreba" and calcuating the derivative being insanely long.@@@It took me a long while too... Both for 4.3 and 4.4. :S@@@2017-06-10T02:28:34Z@@@hw3 student
A3 Q4.4 - Vectorizing the cost function and gradient@@@I'm trying to find a way to write the cost and gradient functions in vector notation. I'm having trouble doing this because while I found an expression for f(W), it involves W. This will create problems if I try to pass it to findMin.findMin unless I change that as well. Should I flatten W and try to obtain an equivalent vector notation form? Or would it be easier to just for-loop it@@@@@@2017-06-09T08:02:44Z@@@hw3 student
Midterm questions@@@Q1 (b) The question is asking "why do we need gradient descent for robust linear regression". However, since robust linear regression use absolute error, I thought the graph of the difference between predicted y and real y will look like a straight line. And we cannot take gradient at 0 because it is a straight line.... Could someone pls explain? (i) How do we know the function has local minima if the function is convex?@@@I think Q1(b) was maybe the most problematic on the whole exam. I don't mean it was a bad question, but that a huge number of students were surprised that their answer was not correct. In particular, most students wrote something about smoothness, which is not what the question is asking. The question is about why do we need gradient descent. The answer is that least squares is a very, very special case where we can (amazingly) write down the solution in "closed form" as $$w=(X^TX-\lambda I)^{-1}X^T y$$ (with L2-regularization). Think about it: for any data set, just plug it in and BAM! The best possible linear model in one line. Wow! Here's an analogy: from high school math, we're given the equation $$ax^2+bx+c=0$$ and we're given the quadratic formula $$x=\frac{-b\pm \sqrt{b^2-4ac}}{2a}$$. Unfortunately high school math teachers make this seem normal. But it is not normal! It is amazing that you can just write down the solution. For the other 99.999999% of equations you can't just do that. For example, how about $$ax^5+bx^4+cx^3+dx^2+ex+f=0$$. Nope, no closed form solution there. So, how to solve it? Well, some sort of iterative procedure! For example Newton's method. Or, sure, gradient descent. So, the thing I really want you to understand is that least squares, like a quadratic equation, is a special case. For most/all other cases, including robust regression, logistic regression, SVM, etc. you can't just write down the solution. So you need to use some sort of iterative method like gradient descent. That's a bit unpleasant becausewe have to deal with an initial guess, a step size, convergence issues, termination conditions, etc. So we'd typically (with some exceptions -- stay tuned) just use the closed form solution if you can. Which, again, is basically only possiblefor least squares. This whole thing has nothing to do with smoothness. For example, logistic regression is smooth, but we need gradient descent. Regarding Q1(i), this comes from the definition of convexity. We're not going into details in CPSC 340. But what I really want you to know is that if a function is convex then every local minimum is a global minimum (and there are no maxima). This doesn't necessarily mean that there is only one minimum. For example, ordinary least squares could have a bunch of solutions with the same loss (this is what we called on-uniqueness). But L2-regularized least squares has a unique solution. Both are convex. The answer, then: we care if something is convex because, if we find a place where $$\nabla f(w)=0$$, then we know there's no other $$w$$ that would have a better (lower) loss.@@@2017-06-09T21:42:25Z@@@midterm student
Getting errors of 1.0 running LogReg@@@When running python main.py -q 3  I get training and test errors of 1.000 which definitely should not happen. I thought  I might have messed with the logReg function so I redownloaded the files and tried again but got them same thing. I asked my tutorial TA about this and he changed "self.w = np.zeros(d)" to "self.w = np.random.randn(d)", which seemed to fix the problem.  Has anyone else  encountered this problem? And is it ok to alter the given logReg code?@@@Interesting that the zero initialization caused problems. Sure, I guess it's fine. But I'm surprised about the error. Curious to hear if anyone else encountered it...@@@2017-06-09T00:31:24Z@@@hw3 student
Q4.4 Implemented gradient almost identical to estimated gradient@@@Running the gradient checker, I always get gradients correct up to the third decimal place... Is this enough? I can't really figure out where I went wrong unless someone could look at the code@@@You just need to reshape it differently. Remember in python a d by 1 is not the same as a vector of length d.@@@2017-06-10T01:07:22Z@@@hw3 student
