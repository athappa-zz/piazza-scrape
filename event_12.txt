2017-06-14 17:35:54.600000
A4: Q2.3@@@How can we implement the training phase in$$O(nd)$$ time instead of$$O(ndck)$$ time?@@@This is one of those unusual cases where the space complexity is more than the time complexity. It's rare since usually you at least need to traverse all the space you use. The space complexity of storing all the conditional probabilities is $$\mathcal{O}(dck)$$. This is the array p_xy in the code. The time complexity of computing all the conditional probabilities, using a straightforward implementation like the one in your assignment, is $$\mathcal{O}(ndck)$$. I agree there. However, to do this you really only need to pass through your data set, which has size $$\mathcal{O}(nd)$$. Basically, for each example and each feature, you look at the feature value itself. And you look at the corresponding y-value. And you use those to index directly into your big array. So you're not looping over the $$c$$ and $$k$$ dimensions, but rather using them to index directly in somewhere. Because at the end of the day you're just counting things and that shouldn't take longer than passing through your data set. As I said before, it's a bit unintuitive because your array has these factors of $$c$$ and $$k$$ in its size but you don't necessarily need to loop over those dimensions. It's sort of like indexing into an array or hash tabletakes constant time even though the size of the thing could be big, if that helps.@@@2017-06-13T21:13:07Z@@@hw4 student
Q3.2@@@In our code when we choose k, do we need to perform procedure described in L24 slide 10? Calculate error with k=0 and compare it with non-zero k error and choose k that gives smallest varienceRemaining (generated dynamically)? Or is k supposed to be hard-coded?@@@Since we want to display data with a scatter plot, I think we should reduce data to just 2 dimensions, thus,k should be picked accordingly.@@@2017-06-15T04:22:10Z@@@hw4 student
A4:Q4@@@I'm trying to take the derivative of the multi-quadric approximation but I can't seem to get the numerator and denominator dimensions to match. Am I doing something wrong?@@@I guess so... I'd suggest taking the partial derivative with respect to one particular element of $$W$$ or $$Z$$ and then try to reassemble things back into a gradient afterwards.@@@2017-06-14T22:46:38Z@@@hw4 student
Q3.1.1@@@To find the first principal component do I pick a random initialization for z and then do alternating minimization?@@@No, that sounds hard!If you subtract the means from both columns then you'llrealize the data set was intentionally picked so that the PCscan be seen by inspection. Nocalculation needed for 3.1.1.@@@2017-06-15T01:52:16Z@@@hw4 student
A4 Q4@@@In the RPCA reconstruction, sometimes I can get a even more clear black shade than the PCA reconstruction. Is this supposed to happen or I am doing something wrong?@@@It looks good to me..@@@2017-06-15T10:19:27Z@@@hw4 student
Principal components and z_i@@@I want to think of the reconstruction of an example as the weighted sum of the principal components where the weights are z_ik. I don't think this is correct because I wrote the matrix equation out and it appears we reconstruct each feature of x-hat separately as the dot-product of the j-th column of W (i.e. the j-th feature of all principal components) and z_i. So we use z_i repeatedly for all features of the reconstructed x_i. Is there an intuitive way to describe PCA in terms of our reconstruction and the learned W & Z? For example, with simple linear regression, we can say the prediction given some x_i is the weighted sum of the elements of x_i.@@@It is the weighted sum of principal components. It just happens to be that the principal components themselves are linear combinations of features, so each PC contributes (potentially, as it could have $$0$$ for a certain feature) to a training example's feature values.@@@2017-06-14T23:34:43Z@@@other student
A4@@@Are we allowed to calculate Frobenius norm using np.linalg.norm() function?@@@Sure, although it's really just np.sum(X**2)@@@2017-06-15T05:59:03Z@@@hw4 student
A4: Q3.3@@@How to get variance explained? I just know that VarianceExplained = 1 - VarianceRemain@@@That's right, and variance remaining is a normalized version of the loss. See L24 slide 10 for the definition.@@@2017-06-14T03:14:21Z@@@hw4 student
PCA alternating minimization@@@With the alternating minimization, we take turns 'playing with' W and Z to optimize the function. I was wondering though, why is Z a variable to begin with? Isn't Z the set of projections of all points of X onto the subspace W?@@@Yes, that is indeed what $$Z$$ represents. The key point here is that to compute the loss you need to know $$W$$ and $$Z$$. So you can't just leave $$Z$$ fixed and do gradient descent w.r.t. $$W$$, because each new $$W$$ value implies a new optimal $$Z$$ value. So you find the best $$Z$$ given $$W$$, then the best $$W$$ given $$Z$$, and so forth. There are variants where you solve the problem to completion each time, i.e. find the *best* $$W$$ given $$Z$$, and there are also variants where you just take a single gradient step: take a descent step in $$W$$ holding $$Z$$ fixed, then a descent step in $$Z$$ holding $$W$$ fixed, etc. You're intuition is right that the values in $$Z$$ is sort of parameters and sort of not parameters. As I said in class, from a ML point of view they aren't really parameters, for the reason you said. But if we forget the interpretation and just treat it as a straight up optimization problem, then we have a bunch of variables (the $$W$$ and $$Z$$ values) that we need to fiddle with in order to minimize the loss. Does that help?@@@2017-06-15T02:58:25Z@@@other student
A4: Q4 Log-sum-exp approximation@@@I noticed in the outline for this question that you recommend to use the "multi-quadric" approximation, however in my implementation I experimented with using the log-sum-exp approximation (as that is what I am familiar with) and my results look pretty good. I suppose my question is: do we need to use the multi-quadric approximation? Or is the log-sum-exp approximation fine for this implementation? Thanks!@@@I suppose that for the assignment, yes you should use the multi-quadratic just so everyone is answering a question of equal difficulty. However, it wouldn't surprise me to hear thata log-sum-exp or Huber approximation works fine. The pros and cons of these different approximations are way beyond the scope of CPSC 340 (and, in fact, the scope of my own knowledge!).@@@2017-06-14T22:48:56Z@@@hw4 student
Lecture 24, Slide 28@@@Why is$$z_i = w^T_cx_i$$? Edit:Never mind, it's explained on slide 20.@@@@259 may help?@@@2017-06-14T21:13:39Z@@@other student
Q1.3 typo?@@@Is "Gaussian likelihood where each datapoint where the variance is sigma 2 instead of 1" meant to say "Gaussian likelihood where the variance is sigma 2 instead of 1" (i.e. the overall sigma 2 ) ?@@@Yes, that's right. Or it couldsay, "We use a Gaussian likelihood for each datapoint where the variance is $$\sigma^2$$ instead of 1"@@@2017-06-13T18:46:11Z@@@hw4 student
Q3.2 Does all count as a Bunch?@@@Question says to label a bunch of points on the scatter plot, not sure if that means there is some specific criteria for choosing what points to label or is it fine to just label all of the points?@@@All is fine.@@@2017-06-14T19:41:21Z@@@hw4 student
Why can we drop the constant in L2 regularization?@@@We have a constant in the NLL of Gaussian prior, but the constant is not included in L2 regularization. Why this is the case?@@@Suppose $$f(w)=w^2+(constant)$$. What is the $$w$$ that minimizes this function? $$0$$. Now, what happens if we change $$f(w)$$ so that $$f(w)=w^2$$ (we get rid of the constant). What is the $$w$$ that minimizes this function? Still $$0$$. So basically, the reason is because the constant doesn't change the minimizer.@@@2017-06-15T01:03:33Z@@@hw4 student
Principal component@@@For A4 3.1, what does it mean to give the first principal component? Are you looking for a vector? An equation of a line? a matrix?@@@It should be the first column of W. W is k by d matrix. Each row(1 by d) in W is a vector of mean for a cluster; Each column(k by 1) is a set of all means for feature'j'.@@@2017-06-14T01:11:37Z@@@hw4 student
Robust PCA@@@Why does robust PCA use absolute error? Isn't the absolute function gonna make it unstable?@@@What does unstable mean? Anyways, I think we use absolute error because the principal components/factors (i.e., the 'means' or the 'bases') are less affected by outliers. If we don't care about the outliers so much, and care onlyabout getting the projection of the non-outlier-points onto the components/factors as close as possible, we want to use a measure of error that's more robust to outliers - i.e., absolute errors. Then the compression will still occur, but in such a way that it's more accurate with the non-outlier points. (Note: PCA is very sensitive to outliers if using euclidean distance/squared error). Absolute function is still convex, so while there's a lack of smoothness that allows us to simply take the derivative/gradient to immediately find minimization point via a closed-form answer, we can use gradient descent to eventually approach the minimization point.@@@2017-06-14T07:22:55Z@@@other student
Naive Bayes for any number of classes@@@In L22, slide 20 it discusses how we would do prediction for naive Bayes for any number of classes. I'm a bit confused on how k would come to play in this part. Are we able to disregard it because the probabilities were calculated in the training phase or are we checking the k probabilities for each of the d features?@@@The equation above, $$p(y=c|x_i) \propto \ldots $$, gives you the probability for a particular class $$c$$ where $$c\in\{1,2,\ldots,k\}$$. If you have $$k$$ classes then you have $$k$$ such probabilities and you take the largest one. Does that help?@@@2017-06-14T01:12:15Z@@@hw4 student
choosing hyper-parameter k@@@In class we discussed how the elbow method could be used to determine the hyper-parameter k for k-means. I was wondering, could we construct a loss function that also takes in the input k such that we can optimize for k? Specifically, the issue seams to be that there's a tradeoff between: 1) Too high a k 2) Too high an error But what if we added some form of regularization on k? So something like: minimize: f(k,W) = (error from optimized clustering) + g(k) where g(k) is some function of k with positive first derivative such that increasing the value of k puts a penalty on the function f. Depending on the class of function g, the magnitude of penalty can be large - in increasing order of magnitude, for example, a log, polynomial, or exponential function.@@@This makes sense to me and a quick search turned up at least one result: https://www.cs.cmu.edu/~dpelleg/download/xmeans.pdf I'm really glad to see you thinking along these lines. The course is working!@@@2017-06-14T06:08:21Z@@@other student
A4: Q3.2@@@"... but because of the binary features even a scatterplot matrix will show us almost nothing about the data." What do we mean by "binary features"? The features don't seem to be binary...@@@try print(X) somewhere in your code My command line didn't print all elements in the matrix X, but the bits it printed seems to be either 0 or 1 -suggesting that it actually may be binary@@@2017-06-15T05:25:21Z@@@hw4 student
A4: Q3.2@@@Why do we standardize the features? In particular, why do we want the columns of X to have a variance of 1(I think in class we only discussed adjusting the mean). And if we standardize the features in main.py, there really is no point in subtracting the mean in the fit method, right?@@@We want to just center the mean around 0 in order to apply PCA to the data. PCA tries to "explain the variance in the data". Yeah you don't have to subtract the mean in fit if you do it already in main.@@@2017-06-15T06:43:53Z@@@hw4 student
