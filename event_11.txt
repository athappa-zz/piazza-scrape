2017-06-12 00:03:04.600000
A4 Q1(5) What's the meaning of "very robust"@@@In Q1(5) "We use a (very robust) student t likelihood witha mean of ...." What's the meaning of very robust hear?@@@Very robust would mean that the model is very insusceptible to outliers affecting the fit of our resulting model.Hint, think about the shape of the loss function.@@@2017-06-12T05:05:33Z@@@hw4 student
Kernel trick with polynomials@@@Why does testing cost only O(ndt)? What about taking the inverse of$$K+\lambda{I}$$? Doesn't that cost$$O(n^3)$$?@@@You can do thatsolve at training time since $$K$$ doesn't involve the test examples. So this is done once during training, not repeatedly per test example. Hence the $$\mathcal{O}(n^3)$$ term in the training time complexity.@@@2017-06-11T18:33:33Z@@@other student
A4: Q2.2@@@Hi, I'm a bit confused on why the commented example in the code says # Compute the conditional probabilities i.e.          # p(x(i,j)=1 | y(i)==c) as p_xy         # p(x(i,j)=0 | y(i)==c) as p_xy Isn't it true that p(x(i,j)=1 | y(i)==c) is the same as p(x(j)=1 | y(i)==c) since at example i we're basically finding out p(feature == 1 | y(i)==c) and p(feature == 0 | y(i) == c) Thanks@@@No because x_i is a d by 1 vector, whereas x_ij is the jth feature of that vector.@@@2017-06-13T01:33:23Z@@@hw4 student
A4: Q2.2@@@I'm getting a very high validation error for this question. I think I implemented it correctly. Is it reasonable for the validation error to be so high? It seems unlikely given that for past assignments it has gone down upon "fixing", but I just wanted to make sure...@@@My result with naive Bayes is slightlybetter than the random forest. I guess you have a bug.@@@2017-06-11T03:17:26Z@@@hw4 student
A2 Mechanics@@@For Assignment 2, was there a requirement to have a link to main.py in the README.md file? I didn't get full marks on "mechanics" and I'm trying to figure out why exactly. Thanks!@@@Yes, according to the homework instructions you should have a link to everything you want the markers to look at.@@@2017-06-11T16:57:58Z@@@hw2 student
multinomial logistic regression@@@I am still not quite sure how is this related to multi-label regression? Why don't we sum k classes for multi-label regression?@@@Sorry, I don't understand the question. What do you mean by "sum k classes"? Also, multi-label is different from multi-class. Are you intentionally referring to multi-label as opposed to multi-class?@@@2017-06-12T18:57:46Z@@@other student
A4: Q1.2@@@For Q1.2, do we use both Laplace prior and Laplace likelihood? Or do we just use Laplace likelihood and 'normal distribution' prior?@@@The latter. For each of the parts you keep the initial assumptions except for the change indicated by the question.@@@2017-06-10T22:34:53Z@@@hw4 student
A4 Q2.2 p_xy variable name@@@For Q2.2, I am confused that # Compute the conditional probabilities i.e.         # p(x(i,j)=1 | y(i)==c) as p_xy         # p(x(i,j)=0 | y(i)==c) as p_xy         p_xy = 0.5 * np.ones((D, C, 2))         ''' TODO for Q2.2: replace the above line with the proper code ''' both probabilities are using the same variable name p_xy. If I follow the example to code, then one of them is gonna overwrite. I was wondering how exactly are we supposed to express 2 probabilities using the same variable name? Thanks!@@@p_xy = 0.5 * np.ones((D, C, 2)) You can see that p_xy has three dimensions, you would use the third dimension to express each of the probabilities@@@2017-06-11T06:59:31Z@@@hw4 student
Multinomial Logistic Regression@@@What does something like this mean? Does it mean that the first term is proportional to the second term? If so, why would that be the case? Thanks@@@It means that the two terms are directly proportional. $$\exp(w_c^Tx_i)$$ increases if and only if $$\exp(w_c^Tx_i)/ (\exp(w_c^Tx_i) + 1)$$ increases; and $$\exp(w_c^Tx_i)$$ decreasesif and only if$$\exp(w_c^Tx_i)/ (\exp(w_c^Tx_i) + 1)$$ decreases. This is true because $$exp(w_c^Tx_i)$$ is always positive and the denominator in the left hand side expression is always greater than 1.@@@2017-06-11T20:30:59Z@@@other student
A4 5.1@@@Regarding KNN, in this context am I correct in assuming that a point cannot be considered one of its own nearest neighbours? For example, if we use number of neighbours = 2 and DO allow a point to consider itself as one of its nearest neighbours, then it only has one choice of edge to get through the graph to the next point. Is this correct? Or should it be, as I'm thinking, ignored, so we would actually consider the 2nd and 3rd nearest neighbours when choosing our edge for number of neighbours = 2?@@@You're right, it's the latter. We look for $$k$$ neighbours other than itself. Or $$k+1$$ neighbours including itself.@@@2017-06-13T05:59:44Z@@@hw4 student
