2017-06-07 16:48:32.631579
python RuntimeWarning: overflow encountered in exp@@@How should I deal with this overflow warning? Is it okay to just ignore it?@@@It's probably OK. But can you provide more information, like which part of the assignment you're doing and when the overflow happens? Is it during a softmax calculation?@@@2017-06-07T02:27:48Z@@@hw3 student
A3 4.1 logistic loss@@@For logistic loss, do we need to add the L2-regularization term in the onjective function? Thanks!@@@You're not required to for the assignment. But you could try it out for fun, if you want to!@@@2017-06-07T03:57:59Z@@@hw3 student
Can tutorial slides be posted please?@@@:)@@@Its under tutorial 5.@@@2017-06-07T21:50:46Z@@@other student
A3 Q4.1 "replace one-vs-all model"@@@The Question asks "write a logLinearClassifier class that replaces the squared loss in the one-vs-al model with the logistic loss" Is that old model code be given?@@@Yes, it is in the leastSquaresClassifier class directly above.@@@2017-06-07T18:35:37Z@@@hw4 student
A3 Q3.3 TODO@@@TODO for Q3.3: Fit the model with 'i' added to the features, # then compute the score and update the minScore/minInd 1. What's the "score" and "minInd" here? Is that the smallest loss/validation error? I am a bit confused@@@Whoops, I changed the variables names in the code but forgot to change the comment. minScore refers tominLoss and minInd refers to bestFeature.@@@2017-06-07T18:31:28Z@@@hw4 student
Cross Validation@@@With say 5-fold cross validation, for each time we use a 1/5th of the X, y in our training, do we then test on the 4 other splits combined or each of them separately?@@@It's the other way around. You train on $$\frac{4}{5}^{th}$$ of the training data and test on the remaining fifth.@@@2017-06-07T15:28:07Z@@@hw3 student
A3 Q2.3 Cost of solving a linear system of equations?@@@I'm trying to figure out Q2.3 and feel really confused about what is the cost of solving a linear system of equations. Can anyone explain a little bit about this? Thanks.@@@When we consider regression (with any sort of basis functions), we learn by identifying the vector "w" of weights that minimize some cost function. Long story short, this typically involves solving the normal equation at the very end to obtain "w". The normal equation is: X^T*X*w = X^T*y. We need to solve this equation in order to obtain the "w", and therefore the cost of solving this equation is part of the cost of fitting the model. You can break down the cost of this by seeing what you would need to do at each step. E.g. what does it cost to multiply X^T with X? What is the cost to multiply X^T with y? What is the cost to then solve Ax = b (a general matrix equation). If A is invertible, what is the cost? Hope this helps.@@@2017-06-07T23:55:55Z@@@hw3 student
Q2.3@@@Can we assume that our computer solves the following system of equations: Ax=b via the process of Gaussian elimination? If so, we can easily calculate the runtime of solving this system as a function of the matrix/vector sizes.@@@Yeah. This is also discussed in @219 and in the slides. You can assume the process of solving $$Ax=b$$ takes $$\mathcal{O}(d^3)$$ if $$A$$ is a $$d \times d$$ matrix.@@@2017-06-08T02:22:32Z@@@hw3 student
A3 Q3.2 "different gradient"@@@When I solve the problem, I commend out the check-gradient because I get different gradient by using findMin.findMinL1. And I still get the answer Is that normal that I have different gradient?@@@How do you know thatyou get the correct answer? Typically, if your gradient is incorrect then optimization wouldn't work too well. But there are some exceptions. For example, if you gradient is a scalar multiple of the true gradient then it won't matter much because the step size is chosen adaptively anyway. Maybe you can paste in the numerical gradient and your gradient, so we can see how they differ?@@@2017-06-07T18:38:03Z@@@hw4 student
A3 Q1.2 - Generate Figure@@@I have figured out how to add the bias term but I'm having trouble outputting the figure. I keep getting dimension errors that when I try to plot the model saying that my dimensions that are getting input to predict are notaligned. I tried subsetting the data to just includethe column that we care about and that didn't seem to help either. Does anyone have any suggestions?@@@If you can't extract an answer from Piazza, feel free to open an issue in your a3 repo, describe the problem, and tag @cpsc340-2017/staff. Then one of us will have a look at your specific code.@@@2017-06-07T03:24:07Z@@@hw3 student
A3 Q4.1@@@The question asks to use logistic loss so I am trying to do it with already implemented logReg class. When I fit the model, however, I got an error caused by check_gradient. I wonder how the implemented and estimated gradients are the same when using logisticData dataset, but different when I am trying to fit one vs all.@@@logisticData is a data set. One vs. all is not a data set, it's a model. So I'm a bit confused by your question. Could you explain exactly what you did, and what the numerical gradient is, and what your gradient is?@@@2017-06-08T06:12:29Z@@@hw3 student
A3 Q2.3@@@Hi, For constructing Z distance matrix in RBF do we use training set to multiply with testing set or the other way around, because I am trying to figure out the dimension of Z. Also, would the dimension of w be changing depend on the size of testing data? Is the dimension of w is t*1? Thanks@@@The number of features becomes the number of training examples. So at test time your $$Z$$ matrix has $$n$$ columns, where $$n$$ is the number of training examples. No, the dimension of $$w$$ is independent of the size of the testing data. As always, it's equal the number of features, which is now $$n$$.@@@2017-06-08T03:28:52Z@@@hw3 student
A3 Q2.1@@@Hi, I was wondering if for this part we have to do any changes in the code or is all that is required from us just an explanation? THank you!!!@@@Just an explanation.@@@2017-06-07T22:11:46Z@@@hw3 student
A3: Q4.4 How to get (w_yi)^T in python?@@@Please help, I been stuck for a whileand not making any progress...@@@If w is a vector and y is an vector and i is an integer than something like w[y[i]] should work. I think in the code you'll actually have W as a matrix so you'll have to be a bit more careful to think about the dimensions of W and what you actually want, but that's the general idea. Also note that the utils.load_dataset function does y-=1 after loading the data here. In the raw data the labels go from 1 to 5 but we change them to go from 0 to 4 by subtracting 1. That way we can use the label (0,1,2,3,4) to directly index into a Python array. (In the pastCPSC 340used Matlab which uses 1-based indexing, hence the data set was crafted to be convenient for Matlab).@@@2017-06-07T04:42:45Z@@@hw3 student
Function for 2.2@@@Are we allowed to use the sklearn.utils.shuffle function to shuffle our X and y for 2.2? Thanks!@@@Sure, that sounds reasonable.@@@2017-06-08T00:05:33Z@@@hw3 student
A3 Q2.2, sklearn@@@The question says not to use a library like cross_val_score, but can we use sklearn.model_selection.kfold, as it only does the splitting of the data for us?@@@No@@@2017-06-07T00:36:51Z@@@hw3 student
Multi-class Classification@@@I'm not sure if I missed anything about this in class but I'm confused by how the columns and rows in this diagram are named. Do k and c refer to the same thing? I am also confused as to why do we have d rows in the Y and W matrix? Thanks!@@@I have in my notes that Y actually has dimension NxC. N rows because there are N examples, and C represents the number of classes that the examples can be classified as. The W matrix has d-rows because w_1 is the weight vector for a single model that classifies whether an example is a cat or not a cat. Remember, most of the linear regression models we discussed have weight vector "w" of dimension "dx1" because you make a prediction by taking the dot product of w^T*x_i. If an example x_i has d features, then it follows that w^T should have d values or the dot product won't make sense.@@@2017-06-08T04:52:22Z@@@hw3 student
A3 Q3.1@@@I am confused by how we calculate the gradient in the objective function method. My understanding was that the L2 regularization term was lamda/2 * ||w||**2 whose derivative islamda/2 * w. But when I implement this in my code I get the following exception: Exception: User and numerical derivatives differ: [ 31.69983779 67.21686918 0.81754183 39.16749733 35.19620532] [ 31.32280542 66.81556771 0.70251601 38.81689108 34.76745623] I don't quite understand why my gradient is off by such a small amount.@@@$$\lambda /2 ||w||^2$$. You forgot the squared, which will change your gradient.@@@2017-06-07T22:33:30Z@@@hw3 student
A3 Q3 logReg@@@The Code: +class logReg:  +    # Logistic Regression  +    def __init__(self, verbose=1, maxEvals=100):  +        self.verbose = verbose  +        self.maxEvals = maxEvals  +        self.bias = True  +  +    def funObj(self, w, X, y):  +        yXw = y * X.dot(w)  +  +        # Calculate the function value  +        f = np.sum(np.log(1. + np.exp(-yXw)))  +  +        # Calculate the gradient value  +        res = - y / (1. + np.exp(yXw))  +        g = X.T.dot(res)  +  +        return f, g  +  +    def fit(self,X, y):  +        n, d = X.shape  +  +        # Initial guess  +        self.w = np.zeros(d)  +        utils.check_gradient(self, X, y)  +        (self.w, f) = findMin.findMin(self.funObj, self.w,  +                                      self.maxEvals, X, y, verbose=self.verbose)  +    def predict(self, X):  +        w = self.w  +        yhat = np.dot(X, w)  +  +        return np.sign(yhat) I have trouble understanding the way we call the function "funobj" in class "logReg": 1. When the function "funobj" was defined, it has 4 arguments:      def funObj( self, w, X, y) 2. But when we call"funObj" ,  there are not arguments at all:       findMin.findMin ( self.funObj, ... ) Can you please explain why it is possible ? What value of w, X ,y were passed into self.funObj when we call it ? Thanks !@@@In the line that you talk about, we are not calling $$\texttt{funObj}$$. We are just passing the function as an argument to another function, $$\texttt{findMin.findMin}$$. If you look at $$\texttt{findMin}$$, you'll see that it calls the function as $$\texttt{funObj(w, *args)}$$. The $$\texttt{*args}$$ takes care of the remaining arguments.@@@2017-06-07T05:25:50Z@@@hw3 student
