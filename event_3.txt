2017-06-20 01:32:44.800000
A5: Q3 Data@@@I was just taking a look at Q3 and when I tried to runthe Jupyter notebook I receivedthe following error message: FileNotFoundError: File b'../data/ml-latest-small/ratings.csv' does not exist I went and looked in the data folder and sure enough, there is no subfolder called ml-latest-small containing ratings.csv. I also took a look inside the assignments folder on the Github page and didn't see a zip file for A5. Am I missing something? Where can I find the data?@@@Read the Jupyter file... clear instructions are provided.@@@2017-06-20T06:29:58Z@@@hw5 student
Lecture 28, Slide 5@@@What makes the$$z_{ic}$$ binary?@@@The number will be somewhere between $$0$$ and $$1$$. So we say it is "sort of" binary.@@@2017-06-19T04:01:41Z@@@other student
A5: Q1@@@How can we add a bias variable to sklearn's neural network?@@@The bias variable is already included in the sklearn's neural network model. You can access the values of the bias vector after training the` model `using the following statement: ` model . intercepts_ ` See the attributes section here: http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier@@@2017-06-19T19:48:05Z@@@hw5 student
Logistic regression vs Logistic loss vs Logistic function@@@I have gone over the relevant lecture slides several times, but I am still confused about the following things. Here is what I know The above is taken from L18. Logisticregression attempts to minimize logistic loss Logistic loss looks like this (the green one) Logistic loss and logistic function are not the same thing Logistic function looks like this Here is what I amunsure about, hoping someone can verify my understanding and answer my questions (Please verify if I am correct) There are two ways to think about logistic regression One as the minimizer for the loss function One as thelogistic function And in either case the logistic regressionlooks the same? THanks so much!@@@2 ways: Minimizing the logistic loss. NOT as the logistic function itself. In the second way, you can think of assuming that the likelihood is a logistic likelihood, ie. $$P(y^i \mid x^i, w) = \dfrac{1}{1 + \exp(-y^iw^Tx^i)}$$. Minimizing the logistic loss or maximizing the log logistic likelihood yields logistic regression.@@@2017-06-20T03:43:43Z@@@other student
Lecture Slides Questions@@@I have saved questions over all the previous lecture slides: - L3 page 9: What is the "()" rule? - L4 bonus page 4: Is "degree" meaning depth? What is "VC"? -L15 page 12: Is "do better away from data" about test error? - L15bonus page 16: What is a universally consistent estimator, and where can I read more about it? - L16 page 3: What term is y_i*x_i? -L20 page 13: With the upper equation, are we missing only the lower polynomial terms from the lower equation? Are all polynomial interactions included in the upper equation?@@@() rule means you just predict the mode of the data without any splitting rule. Yes, I would assume degree here means depth. VC stands for Vapnis-Chervonenkis. VC-Dimension is one way of measuring the complexity/capacity of a class of functions. I don't know if I can explain all of it here, but I'm happy to talk to you about this in office hours :) Yes. Between the training examples, our prediction is dependent on the $$\sigma$$ that we choose for the RBF basis. So by adding a bias or polynomial basis, we can potentially do better than just relying on the RBF approximation at points far away from the test examples. [I'm not 100% sure about this one] Universally consistent means that as the number of training examples goes to infinity, the test error converges to the irreducible error for any underlying distribution of the data. An example is KNN, which is mentioned in the (bonus) slides? This is a result of the application of Stone's theorem. I'm not sure where to read more about it in popular ML literature. But I'm sure you can find out more in functional analysis (something like MATH 421) and advanced statistics textbooks. If you want the minimum of $$f(w)$$ to be at $$w = 0$$, then you need $$f'(0) = 0$$. Taking this derivative and setting it to 0 yields the term you're referring to. Yes, with the upper equation we are only missing the lower order terms. I'm not sure what you mean by "all polynomial interactions". The upper equation contains all polynomial terms of degree 4.@@@2017-06-18T11:19:54Z@@@other student
A5: Q1 What is good enough?@@@For this question, what is considered "good enough"? I can play with this all day and get what I perceive to be good, but I don't know what range would be considered acceptable? Thanks.@@@Question is marked on reasoning not coding. So it probably matters less how "good" an implementation is and more how well you can can explain why it is good. If you want something to compare it to, we've had the same dataset for a previous assignment - you could check you training and testing error against the performance you got there.@@@2017-06-21T00:44:03Z@@@hw5 student
A5:Q2@@@Are we allowed to use stuff fromsklearn.preprocessing to preprocess the data? Edit: Also, do we have to plot a graph for this question?@@@Absolutely, use whatever you want. You don't need to create avisualization, and indeed this is harder to do when $$d>1$$. But if there's a visualization that seems interesting, absolutely include it!@@@2017-06-21T06:41:22Z@@@hw5 student
A5: 3.1 - Missing one of the ten methods?@@@The following methods are listed in the Jupyter notebook as methods we will compare. 1. global average rating 2. user average rating 3. movie average rating 4. average of (2) and (3) above 5. linear regression on movie features, globally 6. linear regression on movie features, separately for each user 7. SVD (naively treating missing entries as 0) 8. SVD (treating missing entries as missing, via gradient descent) 9. Combining (8) with (6) 10. Same as (9) but trained using SGD instead of GD However, we're missing one of these (I believe it's #9). Instead, the section for method 9 seems to be method 10 from the list above. Is this how it's supposed to be? Thanks!@@@Whoops you'reright. It's (10) that I removed (the SGD). The rest are there.@@@2017-06-21T02:58:55Z@@@hw5 student
Final exam cheatsheet@@@Is the final exam cheatsheet also 1 page double sided?@@@Yes@@@2017-06-20T00:31:46Z@@@final_exam student
When will assignment grades be updated again?@@@As title says, when will the grades repo be updated again? I still don't have a grade for a3, and a5 is out already.@@@Just posted a3 grades (TAs finished grading last night). They are now working on a4. Sorry about the delay.@@@2017-06-19T07:08:01Z@@@hw3 hw4 other student
