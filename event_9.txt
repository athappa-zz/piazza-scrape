2017-05-31 14:05:03.235294
Density based clustering@@@If I want to calculate the number of reachable points for point A, does point A itself count as a reachable point?@@@I don't think so. It should be points other than itself.@@@2017-05-30T17:34:11Z@@@other student
Cheatsheet@@@Can we print the one page sheet of notes? or do we have to hand write it?@@@No limitations. You can generate it however you wish.@@@2017-05-31T16:37:15Z@@@midterm student
Why are KNN clustering sensitive to normalization?@@@I'm having a hard time visualizing why it would change anything if we multiplied everything by, say, 100. Could someone help me out?@@@Never mind, I got it. Consider multiple features where a feature is on the wrong scale, it'll affects the distances between points@@@2017-06-01T06:07:23Z@@@midterm student
A2: Q 3.3@@@Hi, For a2, q3.3 I was wondering what our final answers should look like? I have gotten to the step where A w = b, for some A and b Should my answer me something along the following lines 'At at minimizer where \nabla f(w) = 0, we have A w = b' or 'w = inverse of A * b' or something else completely?@@@$$Aw=b$$ sounds good.@@@2017-05-31T02:48:55Z@@@hw2 student
RBFs bumps@@@I have a few questions about this graph: (1) What does it tell us that for Gaussian RBFs, no matter what's the w, it will only have one bump? (2) Comparing to Cubic basis, how does it tell us that Gaussian RBFs is better than Cubic basis? (2) How do we know RBFs is a universal approximators?@@@Here are my thoughts: (1)The single bump indicates that a specific gaussian RBF becomes important when you approach the training point that gives rise to that basis function. The further away you are from the other training points, the more likely it is that their basis functions would take on a small or possibly non-zero valueand have little/lesser effect on our predictions of the data. In a way, I suppose it has built-in distance scaling in that the further away you are from point xyz, the smallerits contribution to the overall prediction (subject to, of course, the weighting). (2) I don't think gaussian RBFs are necessarily better than cubic basis functions, although they are certainly more flexible. Which one you use depends on the context of the problem, the data, and what task you wish to perform. I'm a little iffy on (3). Here's what I think, but it kinda contradicts the slide which states gaussian RBF fits "locally". (3) RBFs are universal approximators because weights can be arbitrarily large or small such that its dot product with the basis functions result in a function which can approximate your data quite well. Furthermore, the basis functions take on values in "different ranges", and therefore allow you to model most of the space. Again, you can take arbitrary weights to help approximate more complex curvature. Note that this isn't always possible with other basis functions: e.g. cubic basis is terrible for approximating periodic functions (you'll want sinusoidal basis for those)@@@2017-05-31T04:51:33Z@@@other student
Normal equations@@@Why does solving a dxd linear system cost O(d^3)?@@@You can solve a linear system using Gaussian elimination which has a runtime of $$O(d^3)$$ for a $$d \times d$$ system. Look here if you want a review of how we get that complexity. Basically, for each of the $$d$$ rows we perform row operations with $$O(d)$$ other rows, where each operation takes $$O(d)$$ time. So, overall we get $$O(d^3)$$.@@@2017-05-30T17:47:44Z@@@other student
Decision Tree: parametric vs non-parametric@@@I've seen a few websites that categorize decision trees as being a non-parametric model. However, if I remember correctly we said that the decision tree is a parametric model? What was the reason for that again? Thanks@@@I think it depends on which algorithm you're using to grow the tree. We said that when we have more data for non-parametric models, the number of parameters grows with the data and the models get more complicated. Adecision tree can be non-parametric if you don't define a max-depth (and so it keeps on growing as you provide it with more data).@@@2017-05-31T02:16:15Z@@@other student
L2 norm notation@@@Could you clarify the notation for the l-1 l-2 l-0 l-infinitynorms? - Is ' ||x|| 2 ' just the l-2 norm (i.e. sqrt(x T x)) or is it the l-2 norm squared (i.e. sqrt(x T x) 2 )? What about ' ||x|| 2 2 ' ? - If a norm was written as "||x||" then is it assumed to be the l2-norm?@@@$$||x||^2$$ is the L2 norm of $$x$$, squared. Namely $$\sum_{i=1}^n x_i^2$$ or $$x^T x$$. $$||x||^2$$ is the same thing as $$||x||_2^2$$. If there's no subscript then we assume it's the L2-norm.@@@2017-05-30T22:52:33Z@@@other student
Q2 - Using a different image@@@How could I input a different image into main.py instead of the dog?@@@You can use skimage.io.imread : http://scikit-image.org/docs/dev/api/skimage.io.html#skimage.io.imread Basically, from skimage.io import imread X = imread(PATH_TO_FILE)@@@2017-05-30T22:23:31Z@@@hw2 student
A1, Q3.2@@@For Q3.2 of Assignment 1, I got that depth 8 minimizes the validation error when using the first half for training and depth 6 when using the second half for training. I checked a few random assignments from other people and they seem to have found the same. I was wondering if there's perhaps an error in the official solution (and hence the grading)? (in the official solution the answer is the other way around) Thanks!@@@We'll look into this after the midterm is graded and sorted out.@@@2017-06-01T03:53:07Z@@@hw1 student
Convexity@@@When we talk about convexity of the function g below, do we care about the function being convex w.r.t to w? or with respect to $$w^Tx_i-y_i$$? Does it matter?@@@In general, we care about its properties w.r.t. $$w$$.@@@2017-05-31T23:00:21Z@@@other student
What does it mean when something is sensitive to outliers?@@@I was reading the slides about outlier detection and for graphical outlier detection one example was PCA but it said that it was sensitive to outliers. What does it mean when something that is used for graphical outlier detection is sensitive to outliers?@@@Outliers affect/change the model significantly.@@@2017-05-31T22:01:29Z@@@midterm student
Q2: How to create array when only having np.argmin values?@@@Once we have an array with all the indices of what is needed from means, how do make an array with the same length that contains the actual content from means? Ex: if I had [1,2,3,1,2,2] and that translates to [[1,1,1],[2,2,2],[3,3,3],[1,1,1],[2,2,2],[2,2,2]], how do I arrive at the second array? I guess this is more of a general python question but I'm getting stuck on this for question 2. Thanks!@@@Your number of mean values is equal to your number of clusters, so you won't have the same amount of mean values as pixels. You need to find a way to associate your means with your cluster assignmentsbut do remember that the indexes of your pixels and your cluster assignments are equal, so perhaps a loop will help you make these associations. Hope that helps. Let me know if I misunderstood your question.@@@2017-05-31T04:26:44Z@@@hw2 student
Lecture 8 Graphical Outlier detection@@@Box plot: Only one variable at a time Scatter plot: Only two variables at a time What exactly do we mean by the number of variables? Why are they different for the box plot and scatter plot?@@@A scatter plot shows two features plotted against each other. A box plot just shows the distribution of one feature.@@@2017-05-30T19:09:05Z@@@other student
A2:Q4.1@@@Apply this model to the data containing outliers, setting z = 1 for the first 400 data points and z = 0.1 for the last 100 data points I'm not sure how to plot this, wouldn't I have 2 different lines this way?@@@Your plot should look the same as the one provided in Q4 but the line will be different because the loss function was changed.@@@2017-05-30T22:19:00Z@@@hw2 student
Q4.2@@@I'm having a lot of trouble with this question. Much like 3.3.3 (where we can create a diagonal matrix Z), is there a 'convenient' method we can use?@@@For this question, I personally find it easier to just compute one partial derivative. Try to guess what the other ones look like and assemble them into a vector to form the gradient. To write your answer in matrix/vector notation, try to replace the sums with norms or dot products.@@@2017-05-30T20:04:11Z@@@hw2 student
Gradient descent@@@In the notes it is mentioned that some issues with gradient descent have to do with scaling and units. Could some explain what exactly the issue was? Thanks@@@Can you please refer to the specific place in the notes? Thx.@@@2017-05-30T17:53:52Z@@@other student
K-means Convex@@@I still don't fully understand what convex means. I only know non-convex means the shape is weird looking. In what situation the K-means clusters is non-convex?@@@A set C is convex if $$\forall x, y \in C,\ \alpha \in [0,1].\ \alpha x + (1-\alpha) y \in C$$. This means that if you take any two points $$x$$ and $$y$$ in the set $$C$$ and draw a line segment between them, then all the points on that line segment are also in the set $$C$$. A function $$f : A \to B$$ is convex if $$A$$ is a convex set and $$\forall x, y \in A.\ \alpha \in [0,1].\ f(\alpha x + (1-\alpha)y) \leq \alpha f(x) + (1-\alpha) f(y)$$. This means that the function is, roughly speaking, "bowl shaped". The function always lies below the line segment between any two points on the function. K-means always produces convex clusters, it can never find non-convex clusters. Why? K-means partitions the space into half-spaces. In the figure above, the line between the blue and red region represents the points from which the blue mean and the red mean are equidistant, and similarly for all the other lines. These regions that we get from dividing the space by line segments like these are convex sets. So K-means will always find convex clusters.@@@2017-06-01T03:50:04Z@@@other student
Assignment 1 marks@@@For my marks on assignment 1, the feedback I got was "LATE: assignment not counted for credit". However, I uploaded all my files except the README by the deadline of 11:59pm and I uploaded the README after the deadline (oops). Does that still mean I don't get marks for the assignment?@@@I'm fine accepting it, but I'll have the TAs take off marks under the "mechanics" category for not following the instructions.@@@2017-06-01T03:38:08Z@@@hw1 student
Decision tree splitting (A1: Q 2.4)@@@In A1, Q2.4 our tree stops splitting once both sides of a split have the same mode of the labels. What if instead we kept going? Isn't there a possibility that doing further splits down the road we could still increase the overall accuracy of the tree?@@@@@@2017-05-31T23:32:54Z@@@other student
2016W2midterm Q1(e)@@@I am not sure why OLS is not sensitive to normalization but OLS with L2 regularization is@@@This is a good question -- the answer is a bit subtle. I'll start by saying that L2-regularization isn't on our midterm. We start with OLS. Imagine a feature is multiplied by 1000 because you convert from kilometres to metres. Then, all the values in that column go up by a factor of 1000. The change to the solution is straightforward: the element of $$w$$ corresponding to that feature will be divided by 1000. So that way everything will stay the same. That's why OLS without regularization is not sensitive to scaling. The model you learn and the predictions you make would be unchanged. When you add regularization you have this $$||w||^2_2$$ term. This term is a sum of squares of the elements of $$w$$. If you multiplied or divided one of the elements of $$w$$ by some factor then its importance would change relative to the other elements of $$w$$. So, the solution would change. Imagine for example that originally $$w_1=1$$. The regularization term takes a penalty of $$1^2=1$$ for that weight. And then imagine you divide the feature by 1000 so that $$w_1$$ becomes 1000. Now the regularization term is taking a penalty of $$1000^2=1000000$$ for that weight. That's a huge penalty. With a huge gradient there: you could reduce the penalty a lot by reducing $$w_1$$ a bit, whereas that wasn't true originally. So the model would probably pick a smaller $$w$$ even if it took a hit in training error. Did that make sense? It's really an important point that's worth understanding even if it takes a few tries, so I'm happy to continue the conversation. I also suspect that the lecture on L1 regularization tomorrow may help developthis way of thinking.@@@2017-06-01T03:43:40Z@@@midterm student
Midterm format@@@I have read through the midterm information provided on github and unfortunately I walked into class today as Mike was, I believe, talking about the midterm, but I may have missed if he explained the format. I was wondering what we should expect the midterm to look like in regards to code, written explanations, and mathematical manipulations. Going through the practice midterms, it seems if there is code, it would be writing psuedo code, and the rest is written explanations and calculations/mathematical manipulations. I just want to make sure I don't focus heavily on python code when the focus is on application and the practicality of the algorithms and topics we have covered.@@@He said the recent spring offering (when he taught the course) gives a flavour of what our exam'll be like@@@2017-05-31T06:01:33Z@@@midterm student
Will we be able to keep or get our cheatsheet back?@@@So that we could re-use for the final?@@@Yes@@@2017-05-31T23:57:53Z@@@midterm student
2016W2midterm, Q4a@@@Is the reason why having x unique features gives us x-1 possible splits that there's no points in having a split such as "variable > maximum value of variable"?@@@It is because if you have x values and you want to make two groups (split them), one group must have at least 1 value and the other can have at most (x-1) values. So the possibilities are 1 and (x-1), 2 and (x-2), ..., (x-1) and 1. (x-1) possible splits in total! Any others wouldn't actually split the data, you'd be left with a group of 0 values and a group of x values, which is the same as a single group of x values.@@@2017-05-31T19:02:12Z@@@midterm student
Can a non-parametric model overfit?@@@I was wondering if it's possible to have overfitting on a non-parametric model like KNN, and if so, how? I was thinking that with a parametric model like decision trees, we might overfit by making the tree fit the data too well (make it too deep) in attempt to get a lower training accuracy. But what about for a KNN?I don't see how this could ever have overfitting because I don't think training accuracy means anything in this context. If we assume the test and training data are i.i.d, could it ever overfit? Any help would be appreciated, thanks!@@@(Modified) I believe so. KNN model is easy to get overfitted as k decreases. Say if we choose k = 1 . Then the KNN would be the same as the original data set, which is overfitting for sure.@@@2017-05-31T00:29:57Z@@@other student
Question about Gradient descent (Slides)@@@I understand how gradient descent works but I feel confused about why we can't take the gradient at zero when we try minimize the absolute error?@@@The absolute value function is not differentiable at zero, so we can't take the gradient there.@@@2017-05-31T18:26:37Z@@@midterm student
Adding a linear basis to matrix "X"@@@I have two questions from lecture 13: (1) To model data of an unknown function, we may consider varying the degree of polynomials used to estimate the data. However, the dimension of the weight vector remains "dx1" throughout. I don't understand how this is possible. Suppose we model with a 1 dimension feature vector with n examples, then our model for simple linear regression is y_i = w*x_i. However, if we wished to estimate the same data with quadratic basis, our model is now y_i = w_1*x_i + w_2 * (x_i)^2. So we went from one weigh to two weights. Unless the dimension is referring to each particular w_i, which is scalar for 1-feature data and (dx1) for d-feature data. So we'd actually have a "weight" matrix with p weights, each of dimension (nx1) (2) page 21: what is the effect of adding a linear basis to the modified matrix? With bias "y", we are translating the prediction vertically. Will the linear basis also impart an overall positive or negative trend much like in lecture 13, page 15 (bottom figure)?@@@First, a terminology thing: you should say "fit the data" or "estimate the model parameters" but not "estimate the data". The data are fixed and do not need to be estimated. (1) We define $$d$$ to be the number of features. So yes $$w$$ has size $$d\times 1$$ but $$d$$ changes as the degree of the polynomial changes. If we use a quadratic basis on a single original feature, then $$d=3$$ because the model is $$y=w_0+w_1x+w_2x^2$$. There is no $$w$$ matrix (until we get to PCA in a couple weeks). (2) Yes, exactly. In addition to the "local" regression we get from RBFs, we also get a global trend. This is particularly relevant far away from the data when the RBF functions tend to zero.@@@2017-06-02T06:26:44Z@@@other student
quantize@@@Is the quantize function actually supposed to return something? What does it mean to return the 'cluster assignments'?@@@"cluster assignments" means an integer label for each point indicating whichcluster it's in. This is basically what Kmeans.predict outputs.@@@2017-05-31T01:37:04Z@@@hw2 student
Q2: Quantize runtime@@@My quantize and dequantize functions seem to be really slow at higher values of k. Will this be a problem when it comes to grading?@@@Hard to say, depends on whether the code is actually correct I guess. You'll find out soon enough!@@@2017-05-31T05:02:46Z@@@hw1 student
Midterm grading concern@@@If we have a concern about our midterm marking, should we wait until Tuesday to submit an issue on GitHub?@@@@197 text in bold@@@2017-06-02T20:51:35Z@@@midterm student
Tutorial slides@@@Could all the tutorial slides be posted please? :) Thanks@@@I've passed this request along to the relevant TAs. Raunak: Slides for 18th May were posted that day itself. For 25th, I worked on the board: I just went through the code for A1, explaining how classes and inheritance works in Python, and I reviewed linear models and normal equations.@@@2017-05-31T18:44:10Z@@@other student
DBScan clusters 'taking' points from others@@@In lecture it was said that in DBScan clusters couldn't 'take' points from other clusters, and that once something is defined as being a part of a cluster it stays that cluster. But in the demo on the outline it looks like they can take them from other groups. https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ Is this a mistake in the demo implementation? Or is this distinction not very important. Also I'm wondering if this could be altered to make it possible for one point to be labeled as a part of multiple clusters (this is stated as missing feature KMeans), meaning the same result would be given regardless of the initialization (for the same hyperparameters).@@@Can you specify which of the demos in that link show this behaviour? I am not sure about your second question but some relevant keywords would be "soft assignments" or "soft clusters" or "overlapping clusters". Some quick Googling makes me think this is possible but not sure about the details. Maybe we could think more about this together some time after the midterm.@@@2017-05-31T22:50:39Z@@@midterm student
Q2: How to print image@@@Are we supposed to use PIL or some other module to display the image? I noticed that the boilerplate we have doesn't import anything to do with an image library, so I was wondering what the correct way would be to display the image with the provided b values. Thanks.@@@It says "You can view the picture by using the plt.imshow function, or by saving it to a file." But this didn't work for me so I opted for the save option by using plt.imsave . Give that a try!@@@2017-05-31T03:37:03Z@@@hw2 student
Do the midterm cover the topics of L2 regularization and Naive Bayes?@@@I think we haven't learned it yet but I saw a lot of questions about this topic on sample midterms.@@@No. I've changed the order of the topics since last year (naive Bayes is coming later) and the midterm is also a little earlier in the term (so L2 regularization is out). See @107.@@@2017-06-01T02:51:42Z@@@midterm student
